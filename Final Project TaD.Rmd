---
title: "Final Project"
author: "Carlo Gre√ü"
date: "2023-11-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(rvest)
library(dplyr)
library(purrr)
```

### Scraping all links available for the years 2003 to 2023
```{r}
url <- "https://www.opec.org/opec_web/en/press_room/307.htm"

# Read the HTML content of the webpage
page <- read_html(url)

# Locate the div with the class "archives" and then select all 'option' elements within it
options <- page %>% html_nodes(".archives option")

# Extract both 'value' attribute and text content
data <- data.frame(
  value = options %>% html_attr("value"),
  year = options %>% html_text()
)

# Removing first row since it is not including a linke
data <- data %>% slice(-1)

# Adding shared first part of the url to all hrefs
data <- data %>% mutate(value = paste0("opec.org", value))

# Print the extracted data
print(data)
```

```{r}
data$value <- paste0("https://www.", data$value)


```


```{r}


# Function to extract links from a single URL
extract_links <- function(url) {
  page <- read_html(url)
  all_links <- page %>% html_nodes('.slim.articles a')
  links <- html_attr(all_links, "href")
  return(links)
}

# Apply the function to each URL in the DataFrame
all_extracted_links <- data %>%
  mutate(extracted_links = map(value, extract_links)) %>%
  pull(extracted_links) %>%
  unlist()
all_extracted_links <- paste0("https://www.opec.org", all_extracted_links)

all_extracted_links <- all_extracted_links[!grepl("^https://www.opec.orghttp://", all_extracted_links)]

# View the modified character vector
print(all_extracted_links)


```

```{r}

# 
# extract_text_from_link <- function(url) {
#   Sys.sleep(2)  # Introduce a 2-second delay
#   webpage <- read_html(url)
#   parsed_nodes <- html_nodes(webpage, xpath = '//p')
#   speech_text <- html_text(parsed_nodes)
#   return(speech_text)
# }
# 
# all_speech_text <- lapply(all_extracted_links, extract_text_from_link)
# 
# head(all_speech_text)
# 
# 
# combined_text <- unlist(all_speech_text)
# 
# # Save the text to a text file
# writeLines(combined_text, "output_text.txt")

```




Code to preserve the years next to the text
```{r}
# Function to extract links from a single URL
extract_links <- function(url) {
  page <- read_html(url)
  all_links <- page %>% html_nodes('.slim.articles a')
  links <- html_attr(all_links, "href")
  return(links)
}

# Apply the function to each URL in the DataFrame
df <- data %>%
  mutate(extracted_links = map(value, extract_links)) %>%
  unnest(extracted_links) %>%
  select(-value)  # Remove the original 'value' column

# View the DataFrame with the extracted links in new rows
print(df)

#Prepend "https://www.opec.org" to each link in the 'extracted_links' column
df <- data %>%
  mutate(extracted_links = map(value, extract_links)) %>%
  unnest(extracted_links) %>%
  mutate(extracted_links = paste0("https://www.opec.org", extracted_links)) %>%
  select(-value)  # Remove the original 'value' column


#removing links that start with "https://www.opec.orghttp://"
df$extracted_links <- lapply(df$extracted_links, function(links) {
  links[!grepl("^https://www.opec.orghttp://", links)]
})

#Filter out rows where the 'extracted_links' list column is empty
df <- df %>%
  filter(map_lgl(extracted_links, ~ length(.x) > 0))

print(df)
```


```{r}
#get text with year
# df <- df %>%
#   filter(map_lgl(extracted_links, ~ length(.x) > 0)) %>%  # Remove rows with empty links
#   mutate(all_speech_text = map(extracted_links, extract_text_from_link)) %>%
#   unnest(all_speech_text) %>%
#   select(-extracted_links)  # Remove the original 'extracted_links' column
# 
# # View the modified DataFrame
# print(df)
# 
# write.csv(df, "Text_years.csv", row.names = FALSE)

```



Code to get the meta-information in separat coloumns

```{r}
extract_text_from_link <- function(url) {
  Sys.sleep(2)  # Introduce a 2-second delay
  webpage <- read_html(url)
  
  # Extract text from the <h1> element
  header_text <- html_text(html_nodes(webpage, "h1"))
  
  # Extract text from the <h5> element
  information_text <- html_text(html_nodes(webpage, "h5"))
  
  # Extract text from all <p> elements and concatenate into a single string
  speech_text <- paste(html_text(html_nodes(webpage, "p")), collapse = " ")
  
  # Check if speech_text is empty
  if (length(speech_text) == 0) {
    cat("No text extracted for URL:", url, "\n")
    return(NULL)
  }
  
  # Create a data frame with the extracted text
  result_df <- data.frame(header = header_text, information = information_text, speech = speech_text)
  
  return(result_df)
}

```

works for subst of data but gives an error when trying with all the links 

```{r}

all_speeches <- df %>%
  filter(map_lgl(extracted_links, ~ length(.x) > 0)) %>%
  mutate(all_speech_text = map(extracted_links, extract_text_from_link)) %>%
  unnest(all_speech_text) %>%
  select(-extracted_links)

# View the test DataFrame
head(all_speeches)
```


